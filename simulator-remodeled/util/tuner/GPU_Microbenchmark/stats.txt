running ./MaxFlops_double microbenchmark
DPU FLOP per SM = 3.999872 (flop/clk/SM)
Total Clk number = 8388876 
/////////////////////////////////
running ./MaxFlops_float microbenchmark
FLOP per SM = 126.245094 (flop/clk/SM)
Total Clk number = 66447 
/////////////////////////////////
running ./MaxFlops_half microbenchmark
half FLOP per SM = 251.186005 (flop/clk/SM)
Total Clk number = 16698 
/////////////////////////////////
running ./MaxFlops_int32 microbenchmark
int32 FLOP per SM = 126.741020 (flop/clk/SM)
Total Clk number = 66187 
/////////////////////////////////
running ./atomic_add_bw microbenchmark
Atomic int32 bandwidth = 0.000576 (byte/clk)
Total Clk number = 14913972232686 
/////////////////////////////////
running ./atomic_add_bw_conflict microbenchmark
Atomic int32 bandwidth = 0.192556 (byte/clk)
Total Clk number = 2788122774 
/////////////////////////////////
running ./atomic_add_lat microbenchmark
Atomic int32 latency = 307.211914 (clk)
Total Clk number = 314585 
/////////////////////////////////
running ./config_dpu microbenchmark
DPU FLOP per SM = 3.999869 (flop/clk/SM)
Total Clk number = 8388883 
double-precision DPU latency = 54.035339 (clk)
Total Clk number = 885315 

//Accel_Sim config: 
-gpgpu_num_dp_units 4
-ptx_opcode_latency_dp 54,54,54,54,330
-ptx_opcode_initiation_dp 64,64,64,64,130
-trace_opcode_latency_initiation_dp 54,64
/////////////////////////////////
running ./config_fpu microbenchmark
FLOP per SM = 126.245094 (flop/clk/SM)
Total Clk number = 66447 
float-precision FPU latency = 4.078552 (clk)
Total Clk number = 66823 

//Accel_Sim config: 
-gpgpu_num_sp_units 4
-ptx_opcode_latency_fp 4,4,4,4,39
-ptx_opcode_initiation_fp 2,2,2,2,4
-trace_opcode_latency_initiation_sp 4,2
/////////////////////////////////
running ./config_int microbenchmark
int32 FLOP per SM = 126.808075 (flop/clk/SM)
Total Clk number = 66152 
int32 latency = 4.166748 (clk)
Total Clk number = 17067 

//Accel_Sim config: 
-gpgpu_num_int_units 4
-ptx_opcode_latency_int 4,4,4,4,21
-ptx_opcode_initiation_int 2,2,2,2,2
-trace_opcode_latency_initiation_int 4,2
/////////////////////////////////
running ./config_sfu microbenchmark
SFU fast sqrt bw = 15.9661(flops/clk/SM) 
Total Clk number = 262701
SFU fast sqrt latency = 23.1262(clk) 
Total Clk number = 94725

//Accel_Sim config: 
-gpgpu_num_sfu_units 4
-ptx_opcode_latency_sfu 23
-ptx_opcode_initiation_sfu 8
-trace_opcode_latency_initiation_sfu 23,8
/////////////////////////////////
running ./config_tensor microbenchmark
wmma PTX issue bandwidth = 1.99833(thread/clk/SM) 
hmma SASS issue bandwidth = 3.99666(thread/clk/SM)
FMA tensor bandwidth = 255.786(FMA/clk/SM)
Total Clk number = 1049452
wmma latency = 64.0752(clk)
hmma latency = 32.0376(clk)
Total Clk number = 262452

//Accel_Sim config: 
-gpgpu_tensor_core_avail 1
-gpgpu_num_tensor_core_units 4
-ptx_opcode_latency_tesnor 64
-ptx_opcode_initiation_tensor 64
-trace_opcode_latency_initiation_tensor 32,32
-specialized_unit_3 1,4,32,4,4,TENSOR
-trace_opcode_latency_initiation_spec_op_3 32,32
/////////////////////////////////
running ./config_udp microbenchmark
-specialized_unit_4 1,4,4,4,4,UDP
-trace_opcode_latency_initiation_spec_op_4 4,1
/////////////////////////////////
running ./core_config microbenchmark
CUDA version number = 8.9

//Accel_Sim config: 
-gpgpu_ptx_force_max_capability 89
-gpgpu_shader_registers 65536
-gpgpu_registers_per_block 65536
-gpgpu_occupancy_sm_number 89
-gpgpu_coalesce_arch 89
-gpgpu_pipeline_widths 4,4,4,4,4,4,4,4,4,4,8,4,4
-gpgpu_sub_core_model 1
-gpgpu_enable_specialized_operand_collector 0
-gpgpu_operand_collector_num_units_gen 8
-gpgpu_operand_collector_num_in_ports_gen 8
-gpgpu_operand_collector_num_out_ports_gen 8
-gpgpu_num_sched_per_core 4
-gpgpu_max_insn_issue_per_warp 1
-gpgpu_dual_issue_diff_exec_units 1
-gpgpu_inst_fetch_throughput 4
-gpgpu_shader_core_pipeline 1536:32
-gpgpu_shader_cta 32
/////////////////////////////////
running ./kernel_lat microbenchmark
Kernel Launch Latency = 218481 cycles
The reported latency above can be slightly higher than real. For accurate evaultion using nvprof event, exmaple: make events ./kernel_lat

//Accel_Sim config: 
-gpgpu_kernel_launch_latency  218480
/////////////////////////////////
running ./l1_access_grain microbenchmark

This benchmark measures coalescing granularity for differnet strides.
check the nvprof or nvsight for received l1 reads and writes.
to run the program with nsight: make nvsight ./l1_access_grain
stats to look at: l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum & l1tex__t_sectors_pipe_lsu_mem_global_op_st.sum

/////////////////////////////////
running ./l1_adaptive microbenchmark
The ubench is not imepleneted yet.
/////////////////////////////////
running ./l1_associativity microbenchmark
Launching L1 cache line size ubench
Saving L1 cache line size data at L1line.csv
Launching L1 cache assoc ubench
Saving L1 cache assoc data at L1asso.csv
/////////////////////////////////
running ./l1_banks microbenchmark
The ubench is not imepleneted yet.
/////////////////////////////////
running ./l1_bw_128 microbenchmark
L1 bandwidth = 117.788(byte/clk/SM), 276.44(GB/s/SM)
Total Clk number = 35609
/////////////////////////////////
running ./l1_bw_32f microbenchmark
L1 bandwidth = 62.9596(byte/clk/SM), 147.762(GB/s/SM)
Total Clk number = 66619
/////////////////////////////////
running ./l1_bw_32f_unroll microbenchmark
L1 bandwidth = 55.493423 (byte/clk/SM)
Total Clk number = 75582 
/////////////////////////////////
running ./l1_bw_64f microbenchmark
L1 bandwidth = 13.4121(byte/clk/SM), 31.4773(GB/s/SM)
Total Clk number = 312725
/////////////////////////////////
running ./l1_bw_64v microbenchmark
L1 bandwidth = 111.06(byte/clk/SM), 260.651(GB/s/SM)
Total Clk number = 18883
/////////////////////////////////
running ./l1_config microbenchmark

//Accel_Sim config: 
-gpgpu_adaptive_cache_config 1
-gpgpu_shmem_option 0,8,16,32,64,100
-gpgpu_unified_l1d_size 128
-gpgpu_l1_banks 4
-gpgpu_cache:dl1 S:4:128:64,L:T:m:L:L,A:384:48,16:0,32
-gpgpu_gmem_skip_L1D 0
-gpgpu_l1_cache_write_ratio 25
/////////////////////////////////
running ./l1_lat microbenchmark
L1 Latency  =      44.4618 cycles
Total Clk number = 1456923 

//Accel_Sim config: 
-gpgpu_l1_latency 44
/////////////////////////////////
running ./l1_mshr microbenchmark
Launching L1 MSHR ubench
Saving L1 MSHR data at MSHR100_array1073741824_shmem12288_itr6.csv
/////////////////////////////////
running ./l1_sector microbenchmark
Launching L1 sector ubench
Saving L1 sector data at data.csv
/////////////////////////////////
running ./l1_shared_bw microbenchmark
Shared Memory Bandwidth = 62.584622 (byte/clk/SM)
Total Clk number = 536145 
/////////////////////////////////
running ./l1_write_policy microbenchmark

This microbenchmark detects L1 write policy.
check the nvprof or nvsight for received l1 reads and writes to detect the policy.
see the code comments for further details
to run the program with nvsight: make nvsight ./l1_write_policy
stats to look at: l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum & l1tex__t_sectors_pipe_lsu_mem_global_op_st.sum & l1tex__t_sectors_pipe_lsu_mem_global_op_ld_lookup_hit.sum & l1tex__t_sectors_pipe_lsu_mem_global_op_st_lookup_hit.sum 

/////////////////////////////////
running ./l2_access_grain microbenchmark

This benchmark measures l2 access granularity for differnet strides.
check the nvprof or nvsight for received l2 reads and write.
to run the program with nsight: make nvsight ./l2_access_grain
stats to look at: lts__t_sectors_srcunit_tex_op_read.sum and lts__t_sectors_srcunit_tex_op_write.sum 

/////////////////////////////////
running ./l2_bw_128 microbenchmark
L2 bandwidth = 1964.67(byte/clk), 4610.94(GB/s)
Max Theortical L2 bandwidth = 1536(byte/clk), 3604.89(GB/s)
L2 BW achievable = 127.908%
Total Clk number = 273263
/////////////////////////////////
running ./l2_bw_32f microbenchmark
L2 bandwidth = 1850.31(byte/clk), 4342.56(GB/s)
Max Theortical L2 bandwidth = 1536(byte/clk), 3604.89(GB/s)
L2 BW achievable = 120.463%
Total Clk number = 580303
/////////////////////////////////
running ./l2_bw_64f microbenchmark
L2 bandwidth = 1698.22(byte/clk), 3985.6(GB/s)
Max Theortical L2 bandwidth = 1536(byte/clk), 3604.89(GB/s)
L2 BW achievable = 110.561%
Total Clk number = 1264552
/////////////////////////////////
running ./l2_config microbenchmark
L2 Cache Size = 72 MB
L2 Banks number = 24

//Accel_Sim config: 
-gpgpu_n_sub_partition_per_mchannel 1
-icnt_flit_size 72
-gpgpu_memory_partition_indexing 0
-gpgpu_cache:dl2 S:1024:128:24,L:B:m:L:P,A:192:4,32:0,32
/////////////////////////////////
running ./l2_copy_engine microbenchmark
L2 Latency no-warmp up =     287.2560 cycles 
Total Clk number = 9412804 
L2 Hit Latency =     286.8539 cycles 
Total Clk number = 9399627 
Is memcpy cached in L2? Yes, error=0.4

//Accel_Sim config: 
-gpgpu_perf_sim_memcpy 1
/////////////////////////////////
running ./l2_lat microbenchmark
L2 Hit Latency =     286.6492 cycles 
Total Clk number = 9392922 
L1 Latency  =      44.5397 cycles
Total Clk number = 1459476 

//Accel_Sim config: 
-gpgpu_l2_rop_latency 241
/////////////////////////////////
running ./l2_write_policy microbenchmark

This microbenchmark detects L2 write policy.
check the nvprof or nvsight for received L2 reads and writes to detect the policy.
see the code comments for further details
to run the program with nvsight: make nvsight ./2
stats to look at: llts__t_sectors_srcunit_tex_op_read.sum & lts__t_sectors_srcunit_tex_op_write.sum & lts__t_sectors_srcunit_tex_op_read_lookup_hit.sum & lts__t_sectors_srcunit_tex_op_write_lookup_hit.sum 

/////////////////////////////////
running ./lat_double microbenchmark
double-precision DPU latency = 54.041809 (clk)
Total Clk number = 885421 
/////////////////////////////////
running ./lat_float microbenchmark
float-precision FPU latency = 4.082275 (clk)
Total Clk number = 66884 
/////////////////////////////////
running ./lat_half microbenchmark
fpu16 latency = 4.195312 (clk)
Total Clk number = 17184 
/////////////////////////////////
running ./lat_int32 microbenchmark
int32 latency = 4.219238 (clk)
Total Clk number = 17282 
/////////////////////////////////
running ./list_devices microbenchmark

Device 0: "NVIDIA GeForce RTX 4090 sm_8.9"
/////////////////////////////////
running ./mem_atom_size microbenchmark

This benchmark measures mem atom size granularity
check the nvprof or nvsight for received mem reads and writes
to run the program with nsight: make nvsight ./l2_access_grain
stats to look at: dram__sectors_read.sum & dram__sectors_write.sum & dram__bytes_read.sum & dram__sectors_read.sum

we launched 37748736 read memory reqs (1 req per thread) with a stride of 32 (128 bytes)
if the number of memory reads is the same as read reqs, then mem atom size is 32B
if the number of memory reads is 2X issued read reqs, then mem atom size is 64B, etc.

/////////////////////////////////
running ./mem_bw microbenchmark
Mem BW= 436.529846 (Byte/Clk)
Mem BW= 832.692672 (GB/sec)
Max Theortical Mem BW= 1008.096008 (GB/sec)
Mem Efficiency = 43.302406 %
Total Clk number = 2075390 
/////////////////////////////////
running ./mem_config microbenchmark
Global memory size = 24 GB
Memory Clock rate = 10501 Mhz
Memory Bus Width = 384 bit
Memory type = GDDR6
Memory channels = 24

//Accel_Sim config: 
-gpgpu_n_mem 24
-gpgpu_n_mem_per_ctrlr 1
-gpgpu_dram_buswidth 2
-gpgpu_dram_burst_length 16
-dram_data_command_freq_ratio 4
-dram_dual_bus_interface 0
-gpgpu_dram_timing_opt nbk=16:CCD=4:RRD=18:RCD=36:RAS=82:RP=36:RC=117:CL=36:WL=12:CDLR=15:WR=36:nbkgrp=4:CCDL=9:RTPL=6
/////////////////////////////////
running ./mem_lat microbenchmark
Mem latency =     560.8494 cycles 
Total Clk number = 4594478 
L2 Hit Latency =     287.8560 cycles 
Total Clk number = 9432466 

//Accel_Sim config: 
-dram_latency 273
/////////////////////////////////
running ./regfile_bw microbenchmark
wmma PTX issue bandwidth = 1.99832(thread/clk/SM) 
hmma SASS issue bandwidth = 3.99664(thread/clk/SM)
FMA tensor bandwidth = 255.785(FMA/clk/SM)
Total Clk number = 1049457

regfile_bw = 1024 (byte/SM)

//Accel_Sim config: 
-gpgpu_num_reg_banks 8
-gpgpu_reg_file_port_throughput 2
/////////////////////////////////
running ./sfu_bw_fsqrt microbenchmark
SFU fast sqrt bw = 15.966(flops/clk/SM) 
Total Clk number = 262703
/////////////////////////////////
running ./sfu_lat_fsqrt microbenchmark
SFU fast sqrt latency = 23.1711(clk) 
Total Clk number = 94909
/////////////////////////////////
running ./shared_bw microbenchmark
Shared Memory Bandwidth = 63.8368(byte/clk/SM), 149.821(GB/s/SM)
Total Clk number = 262814
/////////////////////////////////
running ./shared_bw_64 microbenchmark
Shared Memory Bandwidth = 127.895(byte/clk/SM), 300.161(GB/s/SM)
Total Clk number = 262359
/////////////////////////////////
running ./shared_lat microbenchmark
Shared Memory Latency  = 30.032715 cycles
Total Clk number = 61507 

//Accel_Sim config: 
-gpgpu_smem_latency 30
/////////////////////////////////
running ./shd_config microbenchmark
Shared memory per multiprocessor = 102400 bytes
Shared memory per block = 49152 bytes

//Accel_Sim config: 
-gpgpu_shmem_size 102400
-gpgpu_shmem_sizeDefault 102400
-gpgpu_shmem_per_block 49152
/////////////////////////////////
running ./system_config microbenchmark
Device Name = NVIDIA GeForce RTX 4090
GPU Max Clock rate = 2520 MHz 
GPU Base Clock rate = 2520 MHz 
SM Count = 128
CUDA version number = 8.9

//Accel_Sim config: 
-gpgpu_compute_capability_major 8
-gpgpu_compute_capability_minor 9
-gpgpu_n_clusters 128
-gpgpu_n_cores_per_cluster 1
-gpgpu_clock_domains 2520:2520:2520:5250.5
/////////////////////////////////
running ./tensor_bw_half microbenchmark
FP16 operand, FP32 accumalte:
wmma PTX issue bandwidth = 1.99832(thread/clk/SM) 
hmma SASS issue bandwidth = 3.99664(thread/clk/SM)
FMA tensor bandwidth = 255.785(FMA/clk/SM)
Total Clk number = 1049457

FP16 operand, FP16 accumalte:
wmma PTX issue bandwidth = 3.9954(thread/clk/SM) 
hmma SASS issue bandwidth = 7.99081(thread/clk/SM)
FMA tensor bandwidth = 511.412(FMA/clk/SM)
Total Clk number = 524891
/////////////////////////////////
running ./tensor_lat_half microbenchmark
FP16 operand, FP32 accumalte:
wmma latency = 64.1187(clk)
hmma latency = 32.0593(clk)
Total Clk number = 262630

FP16 operand, FP16 accumalte:
wmma latency = 33.0605(clk)
hmma latency = 16.5303(clk)
Total Clk number = 135416
/////////////////////////////////
